{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3805452c-3261-4b6e-97f7-afdf6d90f883",
   "metadata": {},
   "source": [
    "This script is built to run in python 3.13\n",
    "Step 1: Identify Relevant Databases\n",
    "Since we are working with food packaging materials, we need suspect lists related to:\n",
    "\n",
    "Plastics and additives\n",
    "\n",
    "Contaminants and migrants\n",
    "\n",
    "Food contact chemicals (FCCs)\n",
    "\n",
    "Here is the list of the compiled databases\n",
    " 1. CPPdb ListA Mapped 06032019\n",
    " 2. CPPdb ListB Mapped 06032019\n",
    " 3. ECHA PlasticAdditivesInitiative 06032019\n",
    " 4. FOODCONTACTSDB FCCdb FINAL LIST V5 WStructures\n",
    " 5. FOODPLASTICS 2025 03 31\n",
    " 6. PLASTICMAP 2025 03 31\n",
    " 7. S112 FCCMIGEX\n",
    " 8. S117 PFASFCCDB\n",
    " 9. S118 PFASFCCMIGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb0627-d1ce-444f-abe3-2b0ca4393406",
   "metadata": {},
   "source": [
    "Step 2: Curate the Suspect Lists\n",
    "Standardization pipeline\n",
    "Input columns needed:\n",
    "\n",
    "#Compound name\n",
    "\n",
    "#SMILES\n",
    "\n",
    "#InChIKey\n",
    "\n",
    "#CAS number (optional)\n",
    "\n",
    "#Molecular weight\n",
    "\n",
    "Remove problematic entries:\n",
    "  Mixtures (e.g. multiple SMILES per entry)\n",
    "  Salts or poorly defined substances (you can standardize salts to neutral molecules)\n",
    "  Very large or inorganic compounds (filter MW to e.g. 100–1000 Da)\n",
    "\n",
    "Add metadata like functional use (plasticizer, monomer, photoinitiator, etc.) from the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6d22a-2982-411b-82e2-89908c186a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, SaltRemover\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem import rdMolDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0948e45-d289-4c00-819b-7df6224b1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configure your file paths ===\n",
    "input_files = [\n",
    "    \"CPPdb_ListA_Mapped_06032019.xlsx\",\n",
    "    \"CPPdb_ListB_Mapped_06032019.xlsx\",\n",
    "    \"ECHA_PlasticAdditivesInitiative_06032019.csv\",\n",
    "    \"FOODCONTACTSDB_FCCdb_FINAL_LIST_v5_wStructures.csv\",\n",
    "    \"FOODPLASTICS-2025-03-31.csv\",\n",
    "    \"PLASTICMAP-2025-03-31.csv\",\n",
    "    \"S112_FCCMIGEX.csv\",\n",
    "    \"S117_PFASFCCDB.xlsx\",\n",
    "    \"S118_PFASFCCMIGEX.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b3dd0-c1ee-4e24-954c-d38e80e23ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load files ===\n",
    "all_dfs = []\n",
    "for file in input_files:\n",
    "    ext = os.path.splitext(file)[-1].lower()\n",
    "    if ext in [\".csv\", \".txt\"]:\n",
    "        df = pd.read_csv(file, dtype=str, encoding='utf-8', on_bad_lines='skip')\n",
    "    elif ext in [\".xls\", \".xlsx\"]:\n",
    "        df = pd.read_excel(file, dtype=str)\n",
    "    else:\n",
    "        continue\n",
    "    df[\"Source\"] = os.path.splitext(os.path.basename(file))[0]\n",
    "    all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d3021-d593-4fa5-9d03-13b135dd9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Merge and extract relevant fields ===\n",
    "all_entries = []\n",
    "for df in all_dfs:\n",
    "    for _, row in df.iterrows():\n",
    "        # SMILES: from any of the following columns\n",
    "        smiles = (\n",
    "            row.get(\"SMILES\")\n",
    "        )\n",
    "        if not isinstance(smiles, str) or \".\" in smiles:\n",
    "            continue  # Skip mixtures or missing\n",
    "\n",
    "        # Name / Preferred Name\n",
    "        name = (\n",
    "            row.get(\"PREFERRED NAME\") or row.get(\"Preferred_Name\")\n",
    "        )\n",
    "\n",
    "        # Functional use (if available)\n",
    "        function_fields = [\n",
    "            row.get(\"Function\"),\n",
    "            row.get(\"FunctionalUse\"),\n",
    "            row.get(\"Description, \\nfunction\"),\n",
    "            row.get(\"Other function in plastic\"),\n",
    "            row.get(\"Use \\nlevels and migration potential\")\n",
    "        ]\n",
    "        function = \"; \".join(str(f).strip() for f in function_fields if isinstance(f, str) and f.strip())\n",
    "\n",
    "        all_entries.append({\n",
    "            \"SMILES\": smiles.strip(),\n",
    "            \"Name\": str(name).strip() if isinstance(name, str) else None,\n",
    "            \"Function\": function,\n",
    "            \"Source\": row.get(\"Source\")\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79090c-d9e8-4b68-91bd-3d07846ee390",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_entries).to_csv(\"Raw_Suspect_List.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b861f74-4bfa-432b-9954-0c967c1ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Deduplicate raw SMILES before RDKit processing ===\n",
    "print(f\"🔍 Total entries before raw SMILES deduplication: {len(all_entries)}\")\n",
    "\n",
    "# Convert to DataFrame to drop duplicates based on 'SMILES'\n",
    "df_raw = pd.DataFrame(all_entries)\n",
    "df_raw = df_raw.drop_duplicates(subset=\"SMILES\", keep=\"first\")\n",
    "\n",
    "# Reconvert to list of dicts for the RDKit loop\n",
    "all_entries = df_raw.to_dict(orient=\"records\")\n",
    "\n",
    "print(f\"✅ Entries after removing duplicate SMILES: {len(all_entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d444a2-363e-4310-9c02-2f48f4f87237",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mixture_count = sum(1 for entry in all_entries if \".\" in entry[\"SMILES\"])\n",
    "print(f\"🧼 SMILES containing mixtures (before RDKit): {raw_mixture_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf74b9-9e9b-487f-97b5-5041bf879a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === RDKit processing ===\n",
    "processed = []\n",
    "unique_smiles = set()\n",
    "remover = SaltRemover.SaltRemover()\n",
    "uncharger = rdMolStandardize.Uncharger()\n",
    "#tautomer_canon = rdMolStandardize.TautomerCanonicalizer()\n",
    "\n",
    "for entry in all_entries:\n",
    "    smiles = entry[\"SMILES\"]\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        # Handle mixtures\n",
    "        frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "        if len(frags) > 1:\n",
    "            mol = max(frags, key=lambda m: m.GetNumAtoms())\n",
    "\n",
    "        # Normalize tautomers and neutralize\n",
    "        mol = rdMolStandardize.Normalize(mol)  # Apply normalization directly\n",
    "        mol = uncharger.uncharge(mol)\n",
    "        #mol = tautomer_canon.canonicalize(mol)\n",
    "\n",
    "        # Molecular weight filtering\n",
    "        mw = Descriptors.ExactMolWt(mol)\n",
    "        if not (100 <= mw <= 1000):\n",
    "            continue\n",
    "\n",
    "                # Canonical SMILES\n",
    "        can_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "        if can_smiles in unique_smiles:\n",
    "            continue\n",
    "\n",
    "        unique_smiles.add(can_smiles)\n",
    "\n",
    "        # Get molecular formula\n",
    "        formula = rdMolDescriptors.CalcMolFormula(mol)\n",
    "\n",
    "        entry.update({\n",
    "            \"Canonical_SMILES\": can_smiles,\n",
    "            \"MW\": mw,\n",
    "            \"Formula\": formula\n",
    "        })\n",
    "        processed.append(entry)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# === Save final output ===\n",
    "cleaned_df = pd.DataFrame(processed)\n",
    "\n",
    "# Drop duplicates based on Canonical SMILES, just in case\n",
    "before_dedup = len(cleaned_df)\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=\"Canonical_SMILES\", keep=\"first\")\n",
    "after_dedup = len(cleaned_df)\n",
    "\n",
    "cleaned_df.to_csv(\"Curated_Suspect_List.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done. Final suspect list contains {after_dedup} unique entries.\")\n",
    "print(f\"🧹 Duplicates removed after processing: {before_dedup - after_dedup}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a6663-ba7e-47c0-a06b-40ed3db011b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate accurate mass lists for MS matching\n",
    "\n",
    "#Add the monoisotopic m/z values for the most common adducts:\n",
    "\n",
    "#[M+H]+\tMW + 1.007276\tPositive mode\n",
    "#[M+Na]+\tMW + 22.989218\tPositive mode\n",
    "#[M-H]-\tMW - 1.007276\tNegative mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d67892-6787-4695-b08d-51234183a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Add monoisotopic m/z values for common adducts ===\n",
    "H_mass = 1.007276\n",
    "Na_mass = 22.989218\n",
    "\n",
    "cleaned_df[\"[M+H]+\"] = cleaned_df[\"MW\"].astype(float) + H_mass\n",
    "cleaned_df[\"[M+Na]+\"] = cleaned_df[\"MW\"].astype(float) + Na_mass\n",
    "cleaned_df[\"[M-H]-\"] = cleaned_df[\"MW\"].astype(float) - H_mass\n",
    "\n",
    "# Round to 4 decimals\n",
    "cleaned_df[\"[M+H]+\"] = cleaned_df[\"[M+H]+\"].round(4)\n",
    "cleaned_df[\"[M+Na]+\"] = cleaned_df[\"[M+Na]+\"].round(4)\n",
    "cleaned_df[\"[M-H]-\"] = cleaned_df[\"[M-H]-\"].round(4)\n",
    "\n",
    "# Save final list for MS matching\n",
    "cleaned_df.to_csv(\"Curated_Suspect_List_with_Adducts.csv\", index=False)\n",
    "print(\"✅ Saved: Curated_Suspect_List_with_Adducts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277d1ae-235d-4f76-ba5e-9e534219a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match Loads MS-DIAL and suspect list files in positive ionization results\n",
    "\n",
    "#   Match suspect [M+H]+ to MS-DIAL Average Mz within ±5 ppm\n",
    "\n",
    "#   Keep all matches (1:m (one feature may match multiple suspects) or m:1 (many features to match the same suspect))\n",
    "\n",
    "#   Retain useful metadata from both files (e.g. Alignment ID, MS/MS spectrum, formula, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb1ab0-bef9-4df5-b914-d5c87f3e1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load files ===\n",
    "features_file = \"Aligned_MS1_Features_PI_MSDIAL_03.csv\"\n",
    "suspects_file = \"Curated_Suspect_List_with_Adducts.csv\"\n",
    "\n",
    "features_df = pd.read_csv(features_file, low_memory=False)\n",
    "suspects_df = pd.read_csv(suspects_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fbb7d-7892-4adf-8720-f1d4cc2f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Prepare for matching ===\n",
    "ppm_tolerance = 5\n",
    "matches = []\n",
    "\n",
    "# Make sure m/z columns are numeric\n",
    "features_df[\"Average Mz\"] = pd.to_numeric(features_df[\"Average Mz\"], errors=\"coerce\")\n",
    "suspects_df[\"[M+H]+\"] = pd.to_numeric(suspects_df[\"[M+H]+\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "features_df = features_df.dropna(subset=[\"Average Mz\"])\n",
    "suspects_df = suspects_df.dropna(subset=[\"[M+H]+\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94829c-ddd0-4916-a053-6fb7f69e47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Perform m/z matching ===\n",
    "for _, feature in features_df.iterrows():\n",
    "    feature_mz = feature[\"Average Mz\"]\n",
    "    mz_min = feature_mz - (feature_mz * ppm_tolerance / 1e6)\n",
    "    mz_max = feature_mz + (feature_mz * ppm_tolerance / 1e6)\n",
    "\n",
    "    matches_subset = suspects_df[suspects_df[\"[M+H]+\"].between(mz_min, mz_max)].copy()\n",
    "    \n",
    "    for _, suspect in matches_subset.iterrows():\n",
    "        delta_ppm = abs(feature_mz - suspect[\"[M+H]+\"]) / suspect[\"[M+H]+\"] * 1e6\n",
    "        matches.append({\n",
    "            \"Alignment ID\": feature.get(\"Alignment ID\"),\n",
    "            \"Average Mz\": feature_mz,\n",
    "            \"Average Rt(min)\": feature.get(\"Average Rt(min)\"),\n",
    "            \"MS/MS spectrum\": feature.get(\"MS/MS spectrum\"),\n",
    "            \"Peak Height\": feature.filter(like=\"Height\").mean(),  # average across sample columns\n",
    "            \"Suspect Name\": suspect.get(\"Name\"),\n",
    "            \"Function\": suspect.get(\"Function\"),\n",
    "            \"Source\": suspect.get(\"Source\"),\n",
    "            \"Canonical_SMILES\": suspect.get(\"Canonical_SMILES\"),\n",
    "            \"Formula\": suspect.get(\"Formula\"),\n",
    "            \"MW\": suspect.get(\"MW\"),\n",
    "            \"[M+H]+\": suspect.get(\"[M+H]+\"),\n",
    "            \"Δppm\": round(delta_ppm, 2)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e2d8e-4b82-44a3-a1d6-a1a2a65ad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Convert to DataFrame and save ===\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.to_csv(\"Matched_Suspects_PositiveMode.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Matching complete: {len(matches_df)} matches saved to 'Matched_Suspects_PositiveMode.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01e5db-76a8-46d7-99f6-c5215d551989",
   "metadata": {},
   "source": [
    "Predict MS/MS only for suspects that were matched to real features\n",
    "Use Matched_Suspects_PositiveMode.csv\n",
    "\n",
    "🔁 Steps:\n",
    "#Add a CFMID_ID column (Molecule1, Molecule2, …)\n",
    "        #Preserve the original row order and mapping\n",
    "        #Ensure the CFMID_ID stays traceable to the full Matched_Suspects_PositiveMode.csv\n",
    "#Keep only:\n",
    "    #CFMID_ID\n",
    "    #Canonical_SMILES\n",
    "    #Drop duplicate Canonical_SMILES (keep first)\n",
    "    #Export as tab-separated file with no header\n",
    "    #Later: use CFMID_ID to merge predictions back into your full match table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c6381-4878-4b8f-bd85-488c58b2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load match file\n",
    "matches_df = pd.read_csv(\"Matched_Suspects_PositiveMode.csv\")\n",
    "\n",
    "# Step 2: Drop rows without SMILES\n",
    "matches_df = matches_df.dropna(subset=[\"Canonical_SMILES\"])\n",
    "\n",
    "# Step 3: Add CFM-ID before deduplication\n",
    "matches_df[\"CFMID_ID\"] = [\"Molecule\" + str(i + 1) for i in range(len(matches_df))]\n",
    "\n",
    "# Step 4: Save full match table with IDs (to reconnect later)\n",
    "matches_df.to_csv(\"Matched_Suspects_PositiveMode_with_CFMID_ID.csv\", index=False)\n",
    "\n",
    "# Step 5: Deduplicate by Canonical_SMILES for CFM-ID prediction\n",
    "cfmid_input = matches_df.drop_duplicates(subset=[\"Canonical_SMILES\"], keep=\"first\")\n",
    "cfmid_input = cfmid_input[[\"CFMID_ID\", \"Canonical_SMILES\"]]\n",
    "\n",
    "# Step 6: Export tab-separated file (no header)\n",
    "cfmid_input.to_csv(\"CFMID_Input_PosMode_FromMatches.txt\", sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "print(f\"✅ Done! Input file for CFM-ID created with {len(cfmid_input)} unique SMILES.\")\n",
    "print(\"📁 File: CFMID_Input_PosMode_FromMatches.txt\")\n",
    "print(\"📁 Full match file: Matched_Suspects_PositiveMode_with_CFMID_ID.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c2021-b6c8-438f-a19d-7a0ae8da6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and count lines in the final CFM-ID input file\n",
    "with open(\"CFMID_Input_PosMode_FromMatches.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"📦 Total unique SMILES submitted to CFM-ID: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e6967-7517-40ed-928f-0a8b229ad6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember, you are working just with results from positive mode, you will have to repeat this part for the negative mode as well. \n",
    "#To avoid getting this script too long, I moved the part for the negative results into NI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b919cd-eab3-40fb-ae59-fed40c04aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now download docker, and make sure that you have Windows Subsystem for Linux (wsl) installed and updated for docker to run properly\n",
    "#Open docker and search wishartlab/cfmid and click on pull\n",
    "#Then create a foler named cfmid_runner and put the CFMID_Input_PosMode_FromMatches.txt file inside\n",
    "#Open command promt and run the following\n",
    "#docker run --rm -v \"C:/Users/User/Documents/R projects/NIAS/cfmid_runner:/cfmid/public/\" -i wishartlab/cfmid:latest sh -c \"cd /cfmid/public/ && cfm-predict 'CFMID_Input_PosMode_FromMatches.txt' 0.001 /trained_models_cfmid4.0/[M+H]+/param_output.log /trained_models_cfmid4.0/[M+H]+/param_config.txt 1 myout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d7300-4694-46c3-a357-2bbe493a05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Your Predicted CFM-ID Spectra to .msp\n",
    "#The following script will\n",
    "#Loop through all MoleculeX.txt files in the myout folder and extract:\n",
    "        #ID, SMILES, InChiKey, PMass\n",
    "        #Fragment peaks from your selected energy level (e.g. energy1 = 20 eV)\n",
    "        #Format it into proper .msp\n",
    "        #Save to a single file like cfmid_predicted_20eV.msp\n",
    "#You have to run this 3 times, each time for a separate energy level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf94af-861c-4ff6-8c5b-c3e865f777f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "input_folder = \"cfmid_runner/myout\"  # Folder with MoleculeX.txt files\n",
    "output_msp = \"cfmid_predicted_10eV.msp\"\n",
    "energy_level = \"energy0\"  # change to \"energy0\" or \"energy2\" for 10/40 eV\n",
    "\n",
    "entries = []\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.startswith(\"Molecule\"):\n",
    "        with open(os.path.join(input_folder, filename), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        metadata = {\n",
    "            \"Name\": None,\n",
    "            \"SMILES\": None,\n",
    "            \"InChIKey\": None,\n",
    "            \"Formula\": None,\n",
    "            \"PrecursorMZ\": None\n",
    "        }\n",
    "\n",
    "        peaks = []\n",
    "        current_energy = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Extract metadata\n",
    "            if line.startswith(\"#ID=\"):\n",
    "                metadata[\"Name\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#SMILES=\"):\n",
    "                metadata[\"SMILES\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#InChiKey=\"):\n",
    "                metadata[\"InChIKey\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#Formula=\"):\n",
    "                metadata[\"Formula\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#PMass=\"):\n",
    "                metadata[\"PrecursorMZ\"] = float(line.split(\"=\")[1].strip())\n",
    "\n",
    "            # Set energy block (e.g. energy1 = 20 eV)\n",
    "            elif line.lower().startswith(\"energy\"):\n",
    "                current_energy = line.strip().lower()\n",
    "\n",
    "            # Read peaks if we're in the right energy section\n",
    "            elif current_energy == energy_level and re.match(r\"^\\d\", line):\n",
    "                parts = line.split()\n",
    "                try:\n",
    "                    mz = float(parts[0])\n",
    "                    intensity = float(parts[1])\n",
    "                    peaks.append((mz, intensity))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if not peaks or metadata[\"Name\"] is None:\n",
    "            continue\n",
    "\n",
    "        entry = []\n",
    "        entry.append(f\"Name: {metadata['Name']}\")\n",
    "        entry.append(f\"PrecursorMZ: {metadata['PrecursorMZ']:.5f}\")\n",
    "        entry.append(f\"SMILES: {metadata['SMILES']}\")\n",
    "        entry.append(f\"InChIKey: {metadata['InChIKey']}\")\n",
    "        entry.append(f\"Formula: {metadata['Formula']}\")\n",
    "        entry.append(\"Num Peaks: \" + str(len(peaks)))\n",
    "        for mz, intensity in peaks:\n",
    "            entry.append(f\"{mz:.5f}\\t{intensity:.2f}\")\n",
    "\n",
    "        entries.append(\"\\n\".join(entry))\n",
    "\n",
    "# Save .msp\n",
    "with open(output_msp, \"w\") as out_f:\n",
    "    out_f.write(\"\\n\\n\".join(entries))\n",
    "\n",
    "print(f\"✅ Saved {len(entries)} predicted spectra to '{output_msp}' using {energy_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50114b4f-2622-447f-9e51-d853f61b0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"cfmid_runner/myout\"  # Folder with MoleculeX.txt files\n",
    "output_msp = \"cfmid_predicted_20eV.msp\"\n",
    "energy_level = \"energy1\"  # change to \"energy0\" or \"energy2\" for 10/40 eV\n",
    "\n",
    "entries = []\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.startswith(\"Molecule\"):\n",
    "        with open(os.path.join(input_folder, filename), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        metadata = {\n",
    "            \"Name\": None,\n",
    "            \"SMILES\": None,\n",
    "            \"InChIKey\": None,\n",
    "            \"Formula\": None,\n",
    "            \"PrecursorMZ\": None\n",
    "        }\n",
    "\n",
    "        peaks = []\n",
    "        current_energy = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Extract metadata\n",
    "            if line.startswith(\"#ID=\"):\n",
    "                metadata[\"Name\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#SMILES=\"):\n",
    "                metadata[\"SMILES\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#InChiKey=\"):\n",
    "                metadata[\"InChIKey\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#Formula=\"):\n",
    "                metadata[\"Formula\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#PMass=\"):\n",
    "                metadata[\"PrecursorMZ\"] = float(line.split(\"=\")[1].strip())\n",
    "\n",
    "            # Set energy block (e.g. energy1 = 20 eV)\n",
    "            elif line.lower().startswith(\"energy\"):\n",
    "                current_energy = line.strip().lower()\n",
    "\n",
    "            # Read peaks if we're in the right energy section\n",
    "            elif current_energy == energy_level and re.match(r\"^\\d\", line):\n",
    "                parts = line.split()\n",
    "                try:\n",
    "                    mz = float(parts[0])\n",
    "                    intensity = float(parts[1])\n",
    "                    peaks.append((mz, intensity))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if not peaks or metadata[\"Name\"] is None:\n",
    "            continue\n",
    "\n",
    "        entry = []\n",
    "        entry.append(f\"Name: {metadata['Name']}\")\n",
    "        entry.append(f\"PrecursorMZ: {metadata['PrecursorMZ']:.5f}\")\n",
    "        entry.append(f\"SMILES: {metadata['SMILES']}\")\n",
    "        entry.append(f\"InChIKey: {metadata['InChIKey']}\")\n",
    "        entry.append(f\"Formula: {metadata['Formula']}\")\n",
    "        entry.append(\"Num Peaks: \" + str(len(peaks)))\n",
    "        for mz, intensity in peaks:\n",
    "            entry.append(f\"{mz:.5f}\\t{intensity:.2f}\")\n",
    "\n",
    "        entries.append(\"\\n\".join(entry))\n",
    "\n",
    "# Save .msp\n",
    "with open(output_msp, \"w\") as out_f:\n",
    "    out_f.write(\"\\n\\n\".join(entries))\n",
    "\n",
    "print(f\"✅ Saved {len(entries)} predicted spectra to '{output_msp}' using {energy_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753e5a7-f37c-464a-9a60-4353e5db4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"cfmid_runner/myout\"  # Folder with MoleculeX.txt files\n",
    "output_msp = \"cfmid_predicted_40eV.msp\"\n",
    "energy_level = \"energy2\"  # change to \"energy0\" or \"energy2\" for 10/40 eV\n",
    "\n",
    "entries = []\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.startswith(\"Molecule\"):\n",
    "        with open(os.path.join(input_folder, filename), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        metadata = {\n",
    "            \"Name\": None,\n",
    "            \"SMILES\": None,\n",
    "            \"InChIKey\": None,\n",
    "            \"Formula\": None,\n",
    "            \"PrecursorMZ\": None\n",
    "        }\n",
    "\n",
    "        peaks = []\n",
    "        current_energy = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Extract metadata\n",
    "            if line.startswith(\"#ID=\"):\n",
    "                metadata[\"Name\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#SMILES=\"):\n",
    "                metadata[\"SMILES\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#InChiKey=\"):\n",
    "                metadata[\"InChIKey\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#Formula=\"):\n",
    "                metadata[\"Formula\"] = line.split(\"=\")[1].strip()\n",
    "            elif line.startswith(\"#PMass=\"):\n",
    "                metadata[\"PrecursorMZ\"] = float(line.split(\"=\")[1].strip())\n",
    "\n",
    "            # Set energy block (e.g. energy1 = 20 eV)\n",
    "            elif line.lower().startswith(\"energy\"):\n",
    "                current_energy = line.strip().lower()\n",
    "\n",
    "            # Read peaks if we're in the right energy section\n",
    "            elif current_energy == energy_level and re.match(r\"^\\d\", line):\n",
    "                parts = line.split()\n",
    "                try:\n",
    "                    mz = float(parts[0])\n",
    "                    intensity = float(parts[1])\n",
    "                    peaks.append((mz, intensity))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if not peaks or metadata[\"Name\"] is None:\n",
    "            continue\n",
    "\n",
    "        entry = []\n",
    "        entry.append(f\"Name: {metadata['Name']}\")\n",
    "        entry.append(f\"PrecursorMZ: {metadata['PrecursorMZ']:.5f}\")\n",
    "        entry.append(f\"SMILES: {metadata['SMILES']}\")\n",
    "        entry.append(f\"InChIKey: {metadata['InChIKey']}\")\n",
    "        entry.append(f\"Formula: {metadata['Formula']}\")\n",
    "        entry.append(\"Num Peaks: \" + str(len(peaks)))\n",
    "        for mz, intensity in peaks:\n",
    "            entry.append(f\"{mz:.5f}\\t{intensity:.2f}\")\n",
    "\n",
    "        entries.append(\"\\n\".join(entry))\n",
    "\n",
    "# Save .msp\n",
    "with open(output_msp, \"w\") as out_f:\n",
    "    out_f.write(\"\\n\\n\".join(entries))\n",
    "\n",
    "print(f\"✅ Saved {len(entries)} predicted spectra to '{output_msp}' using {energy_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41409995-c36f-4ce1-aba5-4c8701ef1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Load metadata CSV ===\n",
    "meta = pd.read_csv(\"Matched_Suspects_PositiveMode_with_CFMID_ID.csv\", dtype=str)\n",
    "meta = meta.set_index(\"CFMID_ID\")\n",
    "\n",
    "# === Define columns ===\n",
    "smiles_col = \"Canonical_SMILES\"\n",
    "formula_col = \"Formula\"\n",
    "precursor_col = \"[M+H]+\"\n",
    "\n",
    "# === File paths ===\n",
    "input_folder = \"C:/download\"\n",
    "output_msp = \"cfmid_combined_all_energies.msp\"\n",
    "\n",
    "# === Start processing ===\n",
    "entries = []\n",
    "skipped = 0\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.startswith(\"Molecule\") or not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    molecule_id = filename.replace(\".txt\", \"\")\n",
    "    filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "    if molecule_id not in meta.index:\n",
    "        print(f\"⚠️ Skipping {molecule_id} — not in metadata\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        precursor_mz = float(meta.loc[molecule_id, precursor_col])\n",
    "    except:\n",
    "        print(f\"⚠️ Skipping {molecule_id} — missing precursor m/z in metadata\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    peaks = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r\"^\\d\", line):  # Fragment line\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                mz = float(parts[0])\n",
    "                intensity = float(parts[1])\n",
    "                peaks.append((mz, intensity))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    if not peaks:\n",
    "        print(f\"⚠️ Skipping {filename} — no fragments found\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    smiles = meta.loc[molecule_id, smiles_col]\n",
    "    formula = meta.loc[molecule_id, formula_col] if formula_col in meta.columns else \"N/A\"\n",
    "\n",
    "    entry = [\n",
    "        f\"Name: {molecule_id}\",\n",
    "        f\"PrecursorMZ: {precursor_mz:.5f}\",\n",
    "        f\"SMILES: {smiles}\",\n",
    "        f\"Formula: {formula}\",\n",
    "        f\"Num Peaks: {len(peaks)}\"\n",
    "    ]\n",
    "    for mz, intensity in peaks:\n",
    "        entry.append(f\"{mz:.5f}\\t{intensity:.2f}\")\n",
    "\n",
    "    entries.append(\"\\n\".join(entry))\n",
    "\n",
    "# === Write combined output ===\n",
    "with open(output_msp, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    out_f.write(\"\\n\\n\".join(entries))\n",
    "\n",
    "print(f\"✅ Combined spectra saved to '{output_msp}' ({len(entries)} entries)\")\n",
    "print(f\"⚠️ Skipped {skipped} files due to issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b96268-5eff-42f4-8c5c-7ea0caecd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Up until now we were working on the predicted spectra.\n",
    "#Now move on to python 3.10 and script \"extract_msms_from_experimental_spectra\" to create the experimental scpectra .msp and come back for the mathcing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff5121-894b-4001-860b-659e2c9e9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.importing import load_from_msp\n",
    "from matchms.similarity import CosineGreedy\n",
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "exp_file = \"Extracted_MS2_Spectra_PosMode.msp\"\n",
    "cfmid_file = \"cfmid_online_predicted_20eV.msp\"\n",
    "output_csv = \"Spectral_Matches_PosMode_vs_online_predicted_20eV.csv\"\n",
    "\n",
    "# === Load spectra ===\n",
    "experimental = list(load_from_msp(exp_file))\n",
    "predicted = list(load_from_msp(cfmid_file))\n",
    "\n",
    "print(f\"✅ Loaded {len(experimental)} experimental and {len(predicted)} predicted spectra\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4823ef6-1d5c-4b52-93aa-fd3fbb0ed14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matchms.importing import load_from_msp\n",
    "\n",
    "# === Custom cosine similarity function using PPM ===\n",
    "def cosine_similarity_ppm(spec1, spec2, ppm_tol=5):\n",
    "    mz1, intens1 = np.array(spec1.mz), np.array(spec1.intensities)\n",
    "    mz2, intens2 = np.array(spec2.mz), np.array(spec2.intensities)\n",
    "    precursor1 = spec1.get(\"precursor_mz\")\n",
    "    precursor2 = spec2.get(\"precursor_mz\")\n",
    "    precursor_tol = 10  # ppm window around precursor to ignore\n",
    "    \n",
    "    # Normalize intensities to relative scale (0–1)\n",
    "    intens1 = intens1 / intens1.max() if intens1.max() > 0 else intens1\n",
    "    intens2 = intens2 / intens2.max() if intens2.max() > 0 else intens2\n",
    "    \n",
    "    # Sort m/z values\n",
    "    i, j = 0, 0\n",
    "    matched1, matched2 = [], []\n",
    "\n",
    "    while i < len(mz1) and j < len(mz2):\n",
    "        ppm_diff = abs(mz1[i] - mz2[j]) / mz1[i] * 1e6\n",
    "        if ppm_diff <= ppm_tol:\n",
    "            precursor_hit1 = abs(mz1[i] - precursor1) / precursor1 * 1e6 <= precursor_tol if precursor1 else False\n",
    "            precursor_hit2 = abs(mz2[j] - precursor1) / precursor1 * 1e6 <= precursor_tol if precursor1 else False\n",
    "            if precursor_hit1 or precursor_hit2:\n",
    "                i += 1\n",
    "                j += 1\n",
    "                continue  # skip matching the precursor ion\n",
    "            matched1.append(intens1[i])\n",
    "            matched2.append(intens2[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif mz1[i] < mz2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    if len(matched1) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    dot = np.dot(matched1, matched2)\n",
    "    norm1 = np.linalg.norm(matched1)\n",
    "    norm2 = np.linalg.norm(matched2)\n",
    "    cosine_score = dot / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0\n",
    "\n",
    "    return cosine_score, len(matched1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32fb3d-9f69-4778-9101-bdb0279a8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load spectra ===\n",
    "exp_file = \"Extracted_MS2_Spectra_PosMode.msp\"\n",
    "cfmid_file = \"cfmid_predicted_20eV.msp\"\n",
    "output_csv = \"Spectral_Matches_PosMode_vs_20eV_ppm10.csv\"\n",
    "\n",
    "experimental = list(load_from_msp(exp_file))\n",
    "predicted = list(load_from_msp(cfmid_file))\n",
    "\n",
    "print(f\"✅ Loaded {len(experimental)} experimental and {len(predicted)} predicted spectra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4c8d7-718d-480e-979a-356baee3030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Match spectra using PPM-based cosine ===\n",
    "precursor_ppm = 5  # stricter precursor filter\n",
    "fragment_ppm = 5  # used in the cosine similarity function\n",
    "\n",
    "results = []\n",
    "\n",
    "for exp_spec in experimental:\n",
    "    mz_exp = exp_spec.get(\"precursor_mz\")\n",
    "    if mz_exp is None:\n",
    "        continue\n",
    "\n",
    "    for pred_spec in predicted:\n",
    "        mz_pred = pred_spec.get(\"precursor_mz\")\n",
    "        if mz_pred is None:\n",
    "            continue\n",
    "\n",
    "        # Step 1: precursor filter\n",
    "        ppm_diff = abs(mz_exp - mz_pred) / mz_exp * 1e6\n",
    "        if ppm_diff > precursor_ppm:\n",
    "            continue\n",
    "\n",
    "        # Step 2: compare fragments using cosine with ppm tolerance\n",
    "        score, n_matches = cosine_similarity_ppm(exp_spec, pred_spec, ppm_tol=fragment_ppm)\n",
    "        if score > 0.5:\n",
    "            results.append({\n",
    "                \"Feature_ID\": exp_spec.get(\"feature_id\"),\n",
    "                \"Experimental_mz\": mz_exp,\n",
    "                \"RT_min\": exp_spec.get(\"retention_time\"),\n",
    "                \"Predicted_Name\": pred_spec.get(\"name\") or pred_spec.get(\"compound_name\"),\n",
    "                \"Predicted_mz\": mz_pred,\n",
    "                \"SMILES\": pred_spec.get(\"smiles\"),\n",
    "                \"InChIKey\": pred_spec.get(\"inchikey\"),\n",
    "                \"Cosine_Score\": round(score, 4),\n",
    "                \"Num_Matching_Peaks\": n_matches\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5777a-2ece-410d-a38a-b55dc603e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm_tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d6dc0-325f-4f0f-b65a-174903d57771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(\"Cosine_Score\", ascending=False, inplace=True)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"✅ Done! Found {len(df)} matches with cosine > 0.5 at {ppm_tolerance} ppm tolerance\")\n",
    "print(f\"📁 Results saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4b051-aa50-4def-8f55-3a8a9c4f1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Keep only the best match per Feature_ID based on NUM_MATCHING_PEAKS ===\n",
    "best_matches_df = df.sort_values(\"Num_Matching_Peaks\", ascending=False).drop_duplicates(\"Feature_ID\")\n",
    "\n",
    "# Secondary sort by cosine score\n",
    "best_matches_df = best_matches_df.sort_values([\"Num_Matching_Peaks\", \"Cosine_Score\"], ascending=[False, False])\n",
    "\n",
    "# ❌ Remove meaningless perfect matches with only 1 peak\n",
    "best_matches_df = best_matches_df[\n",
    "    ~((best_matches_df[\"Num_Matching_Peaks\"] == 1) & (best_matches_df[\"Cosine_Score\"] == 1.0))\n",
    "]\n",
    "\n",
    "# 🧹 Remove duplicate predicted compounds\n",
    "best_matches_df = best_matches_df.drop_duplicates(subset=\"Predicted_Name\", keep=\"first\")\n",
    "\n",
    "# 💾 Save to CSV\n",
    "best_matches_df.to_csv(\"Top_Matches_By_NumPeaks_PosMode_ppm5ppm10.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final curated matches: {len(best_matches_df)} unique features + predicted compounds retained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52738e3-33d7-49d0-8ff4-05fdf2643344",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Next step is Retip, which I will be running in R. \n",
    "#Retip needs training set with known compounds and their respective Rt to create the model and the suspect screening results to predict the rt based on the proposed structure\n",
    "#Prepare the data for Retip as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0c873-8f7d-4655-a8fd-2c4cf1851896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load files ===\n",
    "matches_file = \"Top_Matches_By_NumPeaks_PosMode_ppm5ppm10.csv\"\n",
    "cfmid_file = \"Matched_Suspects_PositiveMode_with_CFMID_ID.csv\"\n",
    "\n",
    "matches_df = pd.read_csv(matches_file)\n",
    "cfmid_df = pd.read_csv(cfmid_file)\n",
    "\n",
    "# === Standardize key columns ===\n",
    "cfmid_df[\"CFMID_ID\"] = cfmid_df[\"CFMID_ID\"].astype(str).str.strip()\n",
    "matches_df[\"Predicted_Name\"] = matches_df[\"Predicted_Name\"].astype(str).str.strip()\n",
    "\n",
    "# === Merge on CFMID ID ===\n",
    "merged = matches_df.merge(\n",
    "    cfmid_df[[\"CFMID_ID\", \"Canonical_SMILES\"]],\n",
    "    left_on=\"Predicted_Name\",\n",
    "    right_on=\"CFMID_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional: move Canonical_SMILES column next to SMILES for clarity\n",
    "cols = list(merged.columns)\n",
    "if \"SMILES\" in cols and \"Canonical_SMILES\" in cols:\n",
    "    smi_idx = cols.index(\"SMILES\")\n",
    "    cols.insert(smi_idx + 1, cols.pop(cols.index(\"Canonical_SMILES\")))\n",
    "    merged = merged[cols]\n",
    "\n",
    "# === Save result ===\n",
    "merged.to_csv(\"Top_Matches_Annotated_PosMode.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Canonical SMILES added. Final shape: {merged.shape}\")\n",
    "print(\"📁 Saved to: Top_Matches_Annotated_PosMode.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079364d9-a744-4901-85dd-fa726214f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated matches\n",
    "df = pd.read_csv(\"Top_Matches_Annotated_PosMode.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    \"Predicted_Name\", \"Predicted_mz\", \"SMILES\", \"Cosine_Score\", \"Num_Matching_Peaks\"\n",
    "]\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Rename Canonical_SMILES → smiles\n",
    "df = df.rename(columns={\"Canonical_SMILES\": \"SMILES\"})\n",
    "\n",
    "# Save to new CSV\n",
    "df.to_csv(\"suspect_for_retip.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Saved suspect list for Retip prediction: {df.shape[0]} entries\")\n",
    "print(\"📁 File: suspect_for_retip.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a0727-c9be-4f8f-af60-ecc5bebf9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7bab7-e83b-41bc-b46a-1466dac3c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py310-env]",
   "language": "python",
   "name": "conda-env-.conda-py310-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
